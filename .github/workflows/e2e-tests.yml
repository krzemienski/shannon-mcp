name: E2E Tests

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of E2E tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - real
          - claude
          - full
          - edge
          - stress

jobs:
  test:
    name: E2E Tests - ${{ matrix.os }} - Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.11', '3.12']
        exclude:
          # Skip Windows + Python 3.12 for now due to compatibility
          - os: windows-latest
            python-version: '3.12'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better test coverage
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y sqlite3 git
    
    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install sqlite3 git
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv pip install -e ".[dev]"
        uv pip install pytest-html pytest-cov pytest-xdist
    
    - name: Create test directories
      run: |
        mkdir -p test_output
        mkdir -p test_artifacts
    
    - name: Run E2E tests
      env:
        SHANNON_TEST_MODE: "ci"
        PYTEST_XDIST_WORKER_COUNT: "2"
      run: |
        python run_e2e_tests.py \
          --type ${{ github.event.inputs.test_type || 'all' }} \
          --verbose \
          --parallel
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          test_output/
          test_artifacts/
        retention-days: 30
    
    - name: Upload coverage reports
      if: always()
      uses: codecov/codecov-action@v4
      with:
        files: ./test_output/*/coverage.json
        flags: e2e
        name: e2e-${{ matrix.os }}-${{ matrix.python-version }}
    
    - name: Test Report
      if: always()
      uses: dorny/test-reporter@v1
      with:
        name: E2E Test Results - ${{ matrix.os }} - Python ${{ matrix.python-version }}
        path: 'test_output/*/junit-results.xml'
        reporter: java-junit
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Find the latest test results
          const testDirs = fs.readdirSync('test_output').filter(d => d.startsWith('run_'));
          if (testDirs.length === 0) return;
          
          const latestDir = testDirs.sort().reverse()[0];
          const resultsPath = path.join('test_output', latestDir, 'test_results.json');
          
          if (fs.existsSync(resultsPath)) {
            const results = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));
            
            const comment = `## E2E Test Results - ${{ matrix.os }} - Python ${{ matrix.python-version }}
            
            | Metric | Value |
            |--------|-------|
            | Total Tests | ${results.total} |
            | Passed | ${results.passed} ✅ |
            | Failed | ${results.failed} ❌ |
            | Skipped | ${results.skipped} ⏭️ |
            | Success Rate | ${results.success_rate.toFixed(1)}% |
            | Duration | ${results.duration.toFixed(2)}s |
            ${results.coverage ? `| Coverage | ${results.coverage.toFixed(1)}% |` : ''}
            
            [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  stress-test:
    name: Stress Test
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'stress'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv pip install -e ".[dev]"
    
    - name: Run stress tests
      env:
        SHANNON_STRESS_TEST: "true"
      run: |
        python run_e2e_tests.py --type stress --verbose
    
    - name: Analyze performance results
      run: |
        python -c "
        import json
        from pathlib import Path
        
        # Find latest results
        test_dirs = list(Path('test_output').glob('run_*'))
        if not test_dirs:
            print('No test results found')
            exit(1)
        
        latest = sorted(test_dirs)[-1]
        results_file = latest / 'test_results.json'
        
        if results_file.exists():
            with open(results_file) as f:
                results = json.load(f)
            
            print('=== Performance Analysis ===')
            print(f'Duration: {results.get(\"duration\", 0):.2f}s')
            
            # Check for performance regressions
            if results.get('duration', 0) > 300:  # 5 minutes
                print('WARNING: Tests took longer than 5 minutes!')
                exit(1)
        "

  integration-test:
    name: Integration Test with Claude
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install Shannon MCP
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv pip install -e .
    
    - name: Check for Claude binary
      id: claude_check
      run: |
        if command -v claude &> /dev/null; then
          echo "claude_found=true" >> $GITHUB_OUTPUT
          echo "Claude binary found at: $(which claude)"
          claude --version
        else
          echo "claude_found=false" >> $GITHUB_OUTPUT
          echo "Claude binary not found"
        fi
    
    - name: Run Claude integration tests
      if: steps.claude_check.outputs.claude_found == 'true'
      run: |
        python run_e2e_tests.py --type claude --verbose
    
    - name: Mock Claude integration tests
      if: steps.claude_check.outputs.claude_found == 'false'
      run: |
        echo "Running with mock Claude binary..."
        python run_e2e_tests.py --type claude --verbose

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test, stress-test]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: all-results
    
    - name: Generate summary report
      run: |
        echo "# E2E Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Matrix Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Parse all test results
        for dir in all-results/test-results-*/; do
          if [ -d "$dir" ]; then
            os_python=$(basename "$dir" | sed 's/test-results-//')
            echo "### $os_python" >> $GITHUB_STEP_SUMMARY
            
            # Find test results JSON
            results_file=$(find "$dir" -name "test_results.json" | head -1)
            if [ -f "$results_file" ]; then
              total=$(jq -r '.total' "$results_file")
              passed=$(jq -r '.passed' "$results_file")
              failed=$(jq -r '.failed' "$results_file")
              rate=$(jq -r '.success_rate' "$results_file")
              
              echo "- Total: $total" >> $GITHUB_STEP_SUMMARY
              echo "- Passed: $passed ✅" >> $GITHUB_STEP_SUMMARY
              echo "- Failed: $failed ❌" >> $GITHUB_STEP_SUMMARY
              echo "- Success Rate: ${rate}%" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done